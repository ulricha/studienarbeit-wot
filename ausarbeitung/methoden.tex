%% methoden.tex

\chapter{Methoden und Materialien}
\label{ch:Methoden}
%% ==============================

TODO UserID = Benutzerkennung?

%% ==============================
\section{Warum eigene Software?}
%% ==============================
\label{ch:Grundlagen:sec:WarumEigene}
Das bereits in Abschnitt \ref{ch:Grundlagen:sec:RelatedWork} erwähnte
\emph{wotsap}-Projekt berechnet täglich die Struktur des Web of
Trust. Die Daten werden für eine Web-Applikation benutzt, die
grundlegende Statistiken über Schlüssel berechnet und Pfade zwischen
Schlüsseln graphisch darstellt. Zusätzlich werden die Daten aber auch
für weitere Analysen zur Verfügung gestellt. In diesem Abschnitt wird
begründet, warum für die vorliegende Arbeit nicht auf diese Daten
zurückgegriffen wurde, sondern die Datenextraktion selbst vorgenommen
wurde.

Ein Ziel dieser Arbeit ist es, die Struktur des Web of Trust abseits
der grössten starken Zusammenhangskomponente zu
untersuchen. \emph{wotsap} berechnet allerdings nur die Struktur eben
dieser Zusammenhangskomponente. Schlüssel, die nicht in dieser
Komponente enthalten sind, werden nicht beachtet. \emph{wotsap}
beginnt bei mehreren sehr gut vernetzten Schlüsseln, die sicher in der
MSCC liegen. Von diesen ausgehend werden die Signaturen in der Art
einer Breitensuche (rückwärts) verfolgt. Aufgrund dieser Methode
scheint es mit vertretbarem Aufwand nicht möglich, die Extraktion auf
alle Schlüssel auszudehnen.

Der Anwendungszweck der \emph{wotsap}-Daten ist ausschliesslich die
strukturelle Analyses des Netzes. Die über die reine Struktur
hinausgehenden Daten, die über Schlüssel und Signaturen gespeichert
werden, sind dabei auf ein Minimum reduziert: Für Schlüssel werden
ausschliesslich die KeyID und die primäre UserID gespeichert, für
Signaturen der Cert level und die Schlüssel. Diese Reduktion erlaubt
zwar eine sehr kompakte Speicherung der Daten, macht den Datensatz
aber für eine Auswertung weiterer Eigenschaften von Schlüsseln und
Signaturen unbrauchbar. Das verwendete Dateiformat ist ausserdem recht
unflexibel und lässt eine Speicherung weiterer Daten nur mit grösserem
Aufwand zu.

Die \emph{wotsap}-Daten beinhalten nur die zum jeweiligen Zeitpunkt
gültigen Schlüssel und Signaturen. Dieser "`Schnappschuss"' reicht für
die strukturelle Analyse des Graphen aus. Zeitliche Entwicklungen,
beispielsweise die Grösse des Datenbestandes, die Verwendung
bestimmter Verschlüsselungs- und Signaturalgorithmen und die Entwicklung
einzelner Komponenten können damit aber nicht nachvollzogen werden.

Die \emph{wotsap}-Methode liefert also nicht die im Rahmen dieser
Arbeit benötigten Daten. Eine Anpassung der Software würde auf eine
komplette Neuimplementierung hinauslaufen. Ausserdem beruht die
Extraktion der Daten bei \emph{wotsap} auf der veralteten und kaum
mehr benutzten \emph{PKS}-Keyserver-Implementierung (siehe Abschnitt
\ref{ch:Grundlagen:sec:Design:subsec:der-sks-keyserver}).

Darüber hinaus ist allerdings der \emph{wotsap}-Datensatz
fehlerhaft. Diese Fehler werden sowohl durch Fehler in der
Implementierung als auch durch einen fehlerhaften Datenbestand auf dem
verwendeten Keyserver verursacht: Die grösste starke
Zusammenhangskomponente die durch XXX berechnet %FIXME
wurde (MSCC-1), enthält mit dem Stand vom 02.12.2009 ca. 44900
Schlüssel, während der \emph{wotsap}-Datensatz (MSCC-2) vom gleichen
Tag nur ca. 42130 Schlüssel enthält. Die Differenz zwischen den
Datensätzen ergibt einerseits, dass ca. 1000 Schlüssel in MSCC-1
fehlen, die in MSCC-2 vorhanden sind. Eine stichprobenartige Analyse
von 20 Schlüsseln zeigt, dass diese Schlüssel überwiegend durch
Signaturketten an die MSCC angebunden sind, die aufgrund von
widerrufenen Schlüsseln oder Schlüsselteilen unterbrochen sind. Dabei
treten unter anderem Signaturen auf komplett widerrufenen Schlüsseln und
Signaturen auf widerrufenen UserIDs auf. \emph{wotsap} benutzt GnuPG,
um die OpenPGP-Pakete eines Schlüssels zu parsen. Der Code zum Parsen
der GnuPG-Ausgabe ist fehlerhaft und führt dazu, dass \emph{wotsap}
Widerrufssignaturen fäschlicherweise nicht beachtet.

Auf der anderen Seite sind ca. 3980 Schlüssel zwar in MSCC-1
vorhanden, nicht aber in MSCC-2. Um den Grund dafür zu finden, wurde
für 10 zufällig ausgewählte Schlüssel aus dieser Menge eine
Signaturkette (d.h. ein Pfad) zu einem Schlüssel gesucht, der sowohl
in MSCC-1 als auch in MSCC-2 vorhanden ist. Ebenfalls wurde eine Kette
in der anderen Richtung gesucht. Die Signaturen dieser Ketten wurden
mittels GnuPG kryptographisch verifiziert, um sicherzustellen, dass
sie gültig sind. Kann für beide Richtungen jeweils eine Kette
erfolgreich verifiziert werden, so ist der betreffende Schlüssel per
definitionem in der MSCC. Im vorliegenden Fall konnte das für alle
Schlüssel der Stichprobe gezeigt werden. Der Fehler liegt also
wiederrum bei \emph{wotsap}, dass diese Schlüssel fälschlicherweise
ausschliesst. Als Ursache dafür ergab sich in allen betrachteten
Fällen der fehlerhafte Bestand des Keyservers
\emph{wwwkeys.ch.pgp.net}, der von \emph{wotsap} verwendet wird. Die
Schlüssel wurden von \emph{wotsap} nicht in die MSCC-2 übernommen,
weil Teile der dazu notwendigen Signaturketten (komplette Schlüssel
oder einzelne Signaturen) auf dem Keyserver nicht vorhanden
sind. Besonders h\"aufig traten dabei Schl\"ussel auf, deren
Ablaufdatum durch eine neue Selbstsignatur verl\"angert wurde. Dieses
Signaturpaket fehlte auf \emph{wwwkeys.ch.pgp.net}, wodurch der
betreffende Schl\"ussel f\"alschlicherweise als abgelaufen betrachtet
wurde. Diese fehlenden Teile sind aber auf allen Keyservern des
\emph{SKS}-Verbundes %FIXME
vorhanden. Da die Ursache in allen untersuchten F\"allen die gleiche
war, kann angenommen werden, dass der Fehler systematisch ist und
diesen Teil der Diskrepanz zwischen den Datens\"atzen vollst\"andig
erkl\"art. Als Ursache kommen eine mangelhafte Synchronisation
zwischen \emph{wwwkeys.ch.pgp.net} und dem \emph{SKS}-Netzwerk sowie
Fehler in der auf \emph{wwwkeys.ch.pgp.net} verwendeten
\emph{PKS}-Version in Frage.

Die Natur der Fehler legt nahe, dass hauptsächlich solche Schlüssel
fehlen bzw. fälschlich einbezogen wurden, die nur über eine sehr
geringe Anzahl von redundanten Pfaden zur bzw. von der MSCC
verfügen. Gäbe es mehr redundante Pfade, so hätten einzelne
fehlerhafte Informationen (fehlende bzw. fäschlicherweise als gültig
betrachtete Schlüssel oder Signaturen) einen geringeren Einfluss.

Die Anzahl fehlender bzw. fälschlich einbezogener Schlüssel (ca. 9\%
fehlend, ca. 2\% fälschlich einbezogen relativ zu MSCC-1) ist so
gross, dass qualitative Fehler in der Graphenstruktur zu erwarten
sind. Insbesondere Aussagen über die Verteilung von Knotengraden und
ähnliche Aussagen anhand dieser Daten sind mit Vorsicht zu
geniessen. Eine Reihe von Arbeiten verwendet die \emph{wotsap}-Daten
als Beispiel für ein empirisches soziales bzw. Small-World-Netzwerk
\cite{Brondsema2006} \cite{Heikkila2009} \cite{Dell'Amico2007}. Sofern
die Ergebnisse dieser Arbeit auf experimentellen Resultaten aufbauen,
die mit diesen Daten gewonnen wurden, sollten sie daher mit korrekten
Daten überprüft werden. Der Keyserver \emph{wwwkeys.ch.pgp.net} sollte
von Anwendern nicht mehr benutzt werden.

\section{Design und Implementierung}
\label{ch:Grundlagen:sec:Design}

Dieser Abschnitt beschreibt die im Rahmen dieser Arbeit erstellte
Software zur Extraktion und Analyse des Web of Trust. Abschnitt
\ref{ch:Grundlagen:sec:Design:subsec:der-sks-keyserver} beschreibt
zunächst die Keyserver-Software \emph{SKS}, auf die %FIXME
der Extraktionsteil aufbaut. Abschnitt
\ref{ch:Grundlagen:sec:Design:subsec:eigene-software}
beschreibt
dann die entwickelte Software selbst und begründet das Design.

\subsection{Der SKS Keyserver}
\label{ch:Grundlagen:sec:Design:subsec:der-sks-keyserver}

SKS (Synchronizing Key Server) löst die bis dahin verbreitetste
Keyserver-Software PKS (Public Key Server) ab und wird inzwischen auf
so gut wie allen Keyservern benutzt. Mit \emph{pgp.mit.edu} wurde im
Juli 2009 der letzte grosse Keyserver auf SKS umgestellt. PKS benutzt
ein auf E-Mails basierendes Protokoll zur Synchronisation zwischen
Keyservern, das ausgesprochen ineffizient ist. Ausserdem kommt es mit
einigen Bestandteilen des aktuellen OpenPGP-Standards wie
z.B. \emph{PhotoIDs} und mehreren Unterschlüsseln nicht
zurecht. PKS-Keyserver können über ein Webinterface, ein
E-Mail-Interface und das auf HTTP basierende
HKP-Protokoll\footnote{http://tools.ietf.org/html/draft-shaw-openpgp-hkp-00}
abgefragt werden.

Im Unterschied dazu kommunizieren SKS-Keyserver zur Synchronisation
direkt (ohne Umwege über Mailserver) miteinander.  Dazu wird ein
binäres "`Gossip"'-Protokoll benutzt, das auf einem effizienten
Algorithmus zum Abgleich von Datensätzen beruht
\cite{Minsky2003}. SKS unterstützt alle Bestandteile des
OpenPGP-Standards.

SKS ist in der Sprache Objective Caml (OCaml) implementiert und
benutzt die Datenbank Berkeley DB als Datenablage. Ein
Datenbankeintrag besteht dabei aus einem kompletten OpenPGP-Key,
d.h. einer Reihe von OpenPGP-Paketen. SKS ist in zwei Prozesse
aufgeteilt: der \emph{db}-Prozess beantwortet Datenbank-Abfragen für
die verschiedenen Abfragemöglichkeiten (Webinterface, HKP) während der
\emph{recon}-Prozess für den Abgleich der Datenbank mit anderen
Keyservern zuständig ist.

\subsection{Eigene Software}
\label{ch:Grundlagen:sec:Design:subsec:eigene-software}

Die im Rahmen dieser Arbeit entwickelte Software besteht aus zwei
Teilen: Der Extraktionsteil liesst Schlüssel aus der Datenbank eines
Keyservers aus, parst sie, reduziert sie dabei auf die benötigten
Daten und speichert sie in einer Datei ab. Die reduzierten Daten
werden in einer SQL-Datenbank abgelegt und können dort abgefragt
werden. Der zweite Teil besteht aus einer Reihe von Werkzeugen zur
Analyse dieser Daten entsprechend der Zielsetzung dieser Arbeit.

\subsubsection{Datenextraktion}
\label{sec:datenextraktion}

Die Extraktion der Daten ist recht zeitaufwendig. Auf der zur
Verfügung stehenden Hardware (FIXME Poolrechner Hardware) werden
ca. XX Stunden benötigt. Durch die Aufteilung kann die Extraktion der
Daten einmalig auf dem Keyserver vorgenommen werden, während die Daten
anschliessend auf beliebigen Rechnern zur Verfügung stehen.

Der Extraktionsteil wurde direkt in die SKS-Software
integriert. Dadurch ergeben sich mehrere Vorteile: 

\begin{itemize}
\item Das Interface von SKS für den Datenbankzugriff kann
  wiederverwendet werden
\item SKS enthält einen rudimentären OpenPGP-Parser, d.h. eine Reihe
  von Funktionen, die die Byte-Struktur einzelner OpenPGP-Pakete sowie
  die Paket-Struktur des Schlüssels parsen und die darin enhaltenen
  Informationen in Datenstrukturen zugänglich machen. Durch die
  Verwendung dieser Funktionen kann auf eine eigene Implementierung
  eines OpenPGP-Parsers verzichtet werden.
\item Die in SKS definierten Datenstrukturen für die Auswertung von
  OpenPGP-Paketen und weitere Hilfsfunktionen, beispielsweise für den
  Umgang mit OpenPGP-Fingerprints, können verwendet und erweitert werden
\end{itemize}

Derzeit läuft der Extraktionsteil in Form eines eigenen Prozesses, der
einmalig den kompletten Datenbestand ausliest. Dieser Teil könnte aber
auch direkt in den \emph{db}-Prozess integriert werden, so dass der
laufende Keyserver konstant neue oder geänderte Schlüssel reduziert
und in der SQL-Datenbank ablegt. Auf diese Weise könnte ein ständig
aktueller Datenbestand ohne den Aufwand des vollständigen Auslesens
realisiert werden. Diese Möglichkeit wurde im Rahmen dieser Arbeit
nicht verfolgt, kann aber mit geringem Aufwand realisiert werden.

Da der Extraktionsprozess nur lesend auf die SKS-Datenbank zugreift,
kann er die Daten auslesen, während die \emph{recon}- und
\emph{db}-Prozesse laufen. Eine Unterbrechung des SKS-Betriebes ist
nicht notwendig.

Als Sprache für die Implementierung wurde OCaml ausgewählt. Im
Extraktionsteil bestand dafür aufgrund der Integration in SKS keine
Wahl. Der Analyseteil wurde ebenfalls in OCaml implementiert. Da die
dort verwendeten Daten ausschliesslich aus der SQL-Datenbank stammen
und dort nur primitive Datentypen (Strings, Ganz- und
Fliesskommazahlen) gespeichert werden, können aber für diesen Teil
auch problemlos andere Sprachen verwendet werden.

Der Extraktionsteil iteriert über alle in der SKS-Datenbank
enthaltenen Schlüssel. Jeder Schlüssel wird durch die in SKS
enthaltenen Funktionen geparst. Der Extraktionsteil FIXME muss dann
noch
\begin{itemize}
\item entscheiden, ob der Schlüssel zurückgezogen oder abgelaufen
  ist. Dazu werden die Selbstsignaturen auf den einzelnen UserIDs
  betrachtet. Falls zu einer UserID mehrere Selbstsignaturen
  vorliegen, wird entsprechend der Empfehlung im OpenPGP-Standard nur
  die aktuellste verwendet. Liegt ein Rückrufzertifikat vor oder ist
  der Schlüssel abgelaufen, wird das Ablauf- bzw. Rückzugsdatum
  gespeichert.
\item Fremdsignaturen sammeln. Dazu wird jede UserID betrachtet und
  die darauf angebrachten Signaturen ohne Duplikate gespeichert. Hier
  wird ebenfalls nur die neueste Signatur verwendet. Handelt es sich
  bei der neuesten Signatur um eine Rückrufsignatur, wird das Datum
  des Rückrufs gespeichert und zusätzlich die nächstältere Signatur
  gesucht, auf die sich der Rückruf bezieht.
\item den Schlüssel und jede Fremdsignatur auf die benötigten Daten
  reduzieren. Für den Schlüssel sind dies (falls vorhanden) Rückzugs-
  und Ablaufdatum, die KeyID, die Liste aller UserIDs und die
  Information, welche davon die primäre UserID ist, das
  Erstellungsdatum des Schlüssels und der verwendete
  Public-Key-Algorithmus sowie dessen Schlüssellänge.

  Eine Fremdsignatur wird reduziert auf die KeyID des Unterzeichners, 
  den Zertifizierungslevel, das Erstellungsdatum der Signatur,
  falls vorhanden Ablauf- und Rückzugsdatum, den verwendeten
  Signaturalgorithmus und den verwendeten Public-Key-Algorithmus.
\end{itemize}

Grundsätzlich werden Schlüssel nur dann verworfen, wenn sie nicht
parsebar, d.h. keine Abfolge gültiger OpenPGP-Pakete sind. Schlüssel,
die zurückgezogen oder abgelaufen sind, werden unter Angabe des
jeweiligen Datums trotzdem gespeichert. Auf diese Weise ist
sichergestellt, dass der Datensatz möglichst vollständig ist. Für
einen beliebigen Zeitpunkt stehen alle dann gültigen Schlüssel und
Signaturen zur Verfügung. Es kann also gewissermassen ein
"`Snapshot"' des Web of Trust zu einem beliebigen Zeitpunkt
analysiert werden.

Sind alle Schlüssel extrahiert, werden noch solche Signaturen
entfernt, deren erstellender Schlüssel im Datenbestand nicht vorhanden
ist. Ausserdem werden für Signaturen, die von einem Unterschlüssel
erstellt wurden, die KeyID des signierenden Schlüssels auf die des
Hauptschl\"ussels geändert. Dies ist notwendig, weil das hier
verwendete Datenmodell keine Informationen über Unterschlüssel
enthält.

Erwähnt werden muss, dass der Parse-Vorgang nur prüft, ob die
Paketfolge eines Schlüssels dem OpenPGP-Standard entspricht. Es wird
keinerlei kryptographische Verifizierung der Selbst- und
Fremdsignaturen eines Schlüssels vorgenommen. Grundsätzlich können
ohne Probleme Signaturpakete auf einem Schlüssel angebracht und diese
auf einem Keyserver veröffentlicht werden, auch wenn der Ersteller
nicht über das private Schlüsselmaterial für die kryptographische
Signatur verfügt. Keyserver verifizieren keine Signaturen, so dass der
Signaturteil des Pakets mit beliebigem Inhalt gefüllt werden
kann. GnuPG und PGP verifizieren natürlich Signaturen, so dass eine
solches Signaturpaket dort nicht verwendet wird und damit kein
wirkliches Angriffspotential bietet. Eine Möglichkeit für den
Extraktionsteil bestünde darin, die Signaturen jedes
OpenPGP-Schlüssels mit GnuPG verifizieren zu lassen. Dagegen sprechen
zwei Gründe: Einerseits wäre der Aufruf von GnuPG sehr
zeitaufwendig. Für jeden Schlüssel müsste der Schlüssel in einen
GnuPG-Schlüsselring eingefügt werden. Zusätzlich müssten noch alle
Schlüssel, die den jeweiligen Schlüssel signiert haben, einzeln in der
SKS-Datenbank gesucht und in den Schlüsselring eingefügt werden, um
die Signaturen verifizieren zu können. Letztendlich kostet der Aufruf
von GnuPG selbst Zeit. Für die Anzahl der hier verarbeiteten Schlüssel
(2,6 Millionen) scheint dieser Ansatz daher ungeeignet. Sicherlich
sind Schlüssel mit defekten Signaturpaketen auf den Keyservern
vorhanden. Allerdings müsste deren Anzahl sehr gross sein, um
signifikante Änderungen in der Struktur des Graphen und den
statistischen Auswertungen der Schlüsseleigenschaften zu
erreichen. Eine grosse Zahl solcher Pakete scheint aber
unwahrscheinlich, da zumindest unter Sicherheitsaspekten ein Angreifer
damit keinen offensichtlichen Gewinn erreichen kann.

Die so extrahierten Daten werden in einer SQL-Datenbank (PostgresQL)
abgelegt. Auf diese Weise kann darauf verzichtet werden, eigene
Selektionsmechanismen zu implementieren. Stattdessen kann die
gewünschte Datenmenge einfach als SQL-Abfrage formuliert werden. Auf
diese Weise ergibt sich eine deutlich flexiblere
Abfragemöglichkeit. Die Ablage in der Datenbank ist zwar etwas
langsamer als die Daten im Speicher zu halten. Andererseits müssen die
Daten aber nicht jedesmal komplett in den Speicher geladen und dort
gehalten werden. Ausserdem werden effiziente Indexstrukturen der
Datenbank genutzt und müssen nicht selbst implementiert werden.

\subsubsection{Datenauswertung}
\label{sec:datenauswertung}

Die Datenauswertung besteht aus einer Reihe von unabhängigen
Werkzeugen, die die jeweiligen Analyseaufgaben implementieren. Diese
sind als Kommandozeilenprogramme in separaten Prozessen
realisiert. Eine \"Ubersicht \"uber die Aufteilung der Aufgaben auf
die einzelnen Werkzeuge findet sich in Anhang
\ref{cha:analysewerkzeuge}.

Die zur Community-Analyse verwendeten Algorithmen (siehe Abschnitt
\ref{sec:community-analyse}) wurden aus Zeitgr\"unden nicht selbst
implementiert. F\"ur Fastmod FIXME wurde die Implementierung aus dem
Softwarepaket igraph\cite{Csardi2006} verwendet. F\"ur die restlichen
Algorithmen wurden Implementierungen der jeweiligen Autoren verwendet.

\paragraph{Parallelisierung mit MPI}
\label{sec:parall-mitt-mpi}

Die in Abschnitt
\ref{ch:Grundlagen:sec:Netzwerkanalyse:subsec:Statistiken} definierten
Kennzahlen Eccentricity, Durchmesser, Radius, $h$-Nachbarschaften und
charakteristische Distanz ergeben sich s\"amtlich aus den Distanzen in
einem Graphen. Der \"ubliche Ansatz zur Berechnung dieser Kennzahlen
ist, die Distanzmatrix
\begin{equation}
  \label{eq:7}
  D = (d(u, v))_{u, v\in V}
\end{equation}
zu berechnen, die die Distanz $d(u, v)$ in Zeile $u$ und Spalte $v$
enth\"alt\cite{Brinkmeier2004}. Aus dieser Matrix aller Distanzen im
Graphen lassen sich alle oben angef\"uhrten Kennzahlen berechnen. $D$
kann berechnet werden, indem das single-source-shortest-path-Problem
(SSSP) f\"ur alle $n$ Knoten oder das all-pairs-shortest-path-Problem
(APSP) gel\"ost werden. F\"ur den vorliegenden Fall eines
ungewichteten Graphen l\"asst sich SSSP in $\mathcal{O}(n)$ durch
Breitensuche l\"osen. F\"ur die Berechnung der Distanzmatrix ergibt
sich dann eine Komplexit\"at von $\mathcal{O}(nm)$.

Dieser Ansatz ist hier aufgrund der Gr\"osse des vorliegenden Graphen
nicht geeignet. F\"ur diesen gilt $m > n$, so dass die Berechnung
quadratisch in der Anzahl der Knoten ist. Das sequentielle L\"osen von
SSSP f\"ur jeden Knoten auf der zur Verf\"ugung stehenden Hardware
h\"atte einen Zeitaufwand von ca. 32 Stunden erfordert. Das Vorhalten
der quadratischen Distanzmatrix erfordert eine erhebliche Menge
Speicher (ca. 8 GiB, wenn Distanzen durch 4-Byte-Ganzzahlen
repr\"asentiert werden.

Allerdings ist es nicht notwendig, die gesamte Distanzmatrix
vorzuhalten. Eccentricity, $h$-Nachbarschaften und die
durchschnittliche Distanz eines Knotens sind bestimmbar, wenn SSSP
\emph{f\"ur diesen Knoten} gel\"ost wurde. Durchmesser und Radius
ergeben sich allein aus den Eccentricity-Werten f\"ur alle Knoten. Die
Berechnung kann also parallelisiert werden, indem die Knotenmenge $V$
in Teilmengen $V_1, \dots, V_k$ aufgeteilt wird, die jeweils einer
Berechnungseinheit zugewiesen werden. Jede Berechnungseinheit $i$
l\"ost dann SSSP f\"ur alle Knoten aus $V_i$. Die Ergebnisse m\"ussen
dann nur noch kombiniert werden, um Radius, Durchmesser und
charakteristische Distanz des Graphen zu bestimmen.

Dieser Ansatz wurde auf einem Cluster realisiert, dessen Knoten
mittels Message-Passing-Interface (MPI) kommunizieren. Eine zentrale
\emph{Master}-Instanz zerlegt die Knotenmenge des Graphen und schickt
jeder der $k$ \emph{Worker}-Instanzen die Menge $V_k$, f\"ur die diese
zust\"andig ist. Jede \emph{Worker}-Instanz $k$ l\"ost SSSP f\"ur alle
Knoten aus $V_k$ sequentiell und berechnet damit die gesuchten
Kennzahlen. Die Ergebnisse werden an die \emph{Master}-Instanz
\"ubermittelt, die sie kombiniert.

Da die Berechnung f\"ur die einzelnen Knotenmengen $V_k$ komplett
unabh\"angig l\"auft, beschr\"ankt sich die notwendige Kommunikation
zwischen den Instanzen auf das Zuteilen der Knotenmenge zu einer
\emph{Worker}-Instanz und die \"Ubertragung der Ergebnisse zu der
zentralen \emph{Master}-Instanz. Dieser Ansatz skaliert also
ann\"ahernd linear in der Anzahl der Berechnungseinheiten.

Die Berechnung wurde mit 24 \emph{Worker}-Instanzen durchgef\"uhrt,
wobei eine Worker-Instanz auf einem Core einer Quad-Core-Xeon-CPU
ausgef\"uhrt wird. Insgesamt wurden also 7 Clusterknoten benutzt. Die
notwendige Zeit f\"ur das Berechnen s\"amtlicher oben angef\"uhrter
Kennzahlen reduzierte sich damit auf ca. 20 Minuten.

\section{Community-Analyse}
\label{sec:community-analyse}

In Abschnitt \ref{sec:sozi-komp-des} wurden Mechanismen beschrieben,
die zur Entstehung des Web of Trust beitragen. Ausgehend davon kann
die Annahme getroffen werden, dass die Vernetzung wesentlich von zwei
Faktoren beeinflusst wird: Teilnehmer vernetzen sich einerseits
aufgrund ihrer Zugeh\"origkeit zu einer \emph{sozialen Gruppe}. Dabei
kann es sich um Open-Source-Projekte wie z.B. Debian, akademische
Einrichtungen und Firmen handeln, aber auch um eine Gruppe von
Freunden oder Bekannten. Andererseits vernetzen sich Teilnehmer auf
Keysigning-Parties mit einer relativ grossen Anzahl Benutzer, die
ihnen nicht unbedingt bekannt sind und mit denen sie keine starke
Gruppenzugeh\"origkeit verbindet. Offensichtlich ist allerdings, dass
diese Mechanismen sich nicht gegenseitig ausschliessen. Die bereits in
\ref{sec:sozi-komp-des} beschriebene Bandbreite von Keysigning-Parties
reicht von Ad-hoc-Veranstaltungen mit wenigen Teilnehmern, die
durchaus der selben Institution angeh\"orig sein k\"onnen, bis zu
grossen, formalisierten Veranstaltungen auf Konferenzen und Messen mit
etlichen Teilnehmern.

Es soll nun untersucht werden, inwiefern sich diese postulierten
Entstehungsmechanismen in der Struktur, also den topologischen
Eigenschaften, des Web of Trust wiederfinden. Angenommen wird, dass
sich ein hoher Anteil der Teilnehmer prim\"ar anhand dieser beiden
Mechanismen vernetzen und der Anteil an Signaturen zwischen
Teilnehmern, die nicht durch eine Keysigning-Party oder eine soziale
Gruppe entstanden ist, deutlich geringer ist. In diesem Fall ist zu
erwarten, dass sich soziale Gruppen und die Ergebnisse von
Keysigning-Parties als Gruppen von Knoten im Netzwerk wiederfinden,
die untereinander eine hohe Anzahl von Kanten hat, d.h. \emph{dicht}
vernetzt ist, w\"ahrend sich zu Knoten ausserhalb der Gruppe nur
relativ wenige Kanten finden. Diese Eigenschaft entspricht exakt der
in Abschnitt
\ref{ch:Grundlagen:sec:Netzwerkanalyse:subsec:Communities}
beschriebenen Definition von \emph{Communities}. Es erscheint als
Methode deshalb sinnvoll, mit vorhandenen Methoden zur
Community-Erkennung eine Zerlegung des Graphen zu berechnen und dann
zu untersuchen, inwiefern sich die einzelnen so berechneten
Communities entsprechend der Annahme auf soziale Gruppen
bzw. Keysigning-Parties abbilden lassen. Beispielsweise k\"onnten sich
eine (oder mehrere) Communities ergeben, die von Mitgliedern des
Debian-Projekts dominiert werden.

Dazu sind Kriterien n\"otig, um eine berechnete Community einer
sozialen Gruppe, einer Keysigning-Party oder beidem zuzuordnen. F\"ur
Keysigning-Parties ist anzunehmen, dass die Signaturvorg\"ange
zwischen den Teilnehmern einer KSP innerhalb eines eng begrenzten
Zeitraums nach Stattfinden der KSP vorgenommen werden. Als einfaches
Kriterium f\"ur die Erkennung einer Keysigning-Party kann also eine
starke zeitliche Korrelation der Signaturvorg\"ange zwischen der
Mehrzahl der Community-Mitglieder verwendet werden. Konkret wird hier
eine Community als Produkt einer KSP betrachtet, wenn 80\% der
Signaturen zwischen den Mitgliedern innerhalb eines Monats vorgenommen
wurden.

Die Zuordnung zu sozialen Gruppen wird dadurch erschwert, dass --
abgesehen von den Schl\"usseln und Signaturen selbst -- keine
empirischen Daten \"uber die Gruppenzugeh\"origkeit vorliegen. Die
einzigen Daten, die eine (primitive) Zuordnung erlauben, sind die in
den UserIDs enthaltenen E-Mail-Adressen. \"Uber die Top-Level-Domain
(TLD) kann ein User einem Land zugeordnet werden, sofern es sich nicht
um eine der generischen TLDs (com, org, net) handelt. \"Uber die
Second-Level-Domain kann versucht werden, einen Nutzer einer
Institution zuzuordnen. Ein User, der beispielsweise \"uber eine
E-Mail-Adresse mit der Domain debian.org verf\"ugt, ist sicherlich ein
Mitglied des Debian-Projekts. Hilfreich dabei ist, dass Teilnehmer
dazu tendieren, alle ihre Adressen auf ihren Schl\"usseln zu
vermerken, so dass viele Schl\"ussel mehrere UserIDs und
E-Mail-Adressen haben (siehe Abschnitt
\ref{sec:result-key-properties}). Dadurch werden die verf\"ugbaren
Informationen \"uber Gruppenzugeh\"origkeiten erh\"oht. Adressen
verbreiteter ``Freemail''-Anbieter wie Gmail, GMX und Yahoo sowie
Adressen von Internet Service Providern d\"urfen dabei nat\"urlich
nicht verwendet werden, weil ihre Verwendung nichts \"uber eine
Gruppenzugeh\"origkeit aussagt.

\"Ahnlich wie f\"ur Keysigning-Parties werden wieder einfache
Schwellwerte verwendet, um eine Community einer sozialen Gruppe, also
einer Domain, zuzuordnen: Eine Community kann einer Domain
\emph{zugeordnet} werden, wenn mindestens 80\% der Mitglieder
eine UserID mit dieser Domain haben. Ausserdem wird eine Community von
einer Domain \emph{dominiert}, wenn ein erheblicher Anteil -- konkret
mindestens 40\% -- der Mitglieder eine UserID mit dieser Domain haben.

Die Schwellwerte wurden nicht anhand empirischer Daten festgelegt
sondern dr\"ucken nur ein intuitives Verst\"andnis f\"ur die
\"uberwiegende Mehrzahl der Mitglieder einer Gruppe bzw. einen
erheblichen Anteil der Mitglieder einer Gruppe aus.

Wie bereits in Abschnitt
\ref{ch:Grundlagen:sec:Netzwerkanalyse:subsec:Communities}
beschrieben, existiert -- insbesondere f\"ur ungerichtete Graphen --
eine Vielzahl von Methoden zur Zerlegung eines Graphen in
Communities. Dabei hat sich bislang kein ``bester'' Algorithmus
herauskristallisiert, auch wenn einige Algorithmen f\"ur eine Reihe
von Benchmark-Graphen bessere Ergebnisse liefern als andere
\cite{Lancichinetti2009}. Das \"ubliche G\"utemass zum Vergleich von
Community-Zerlegungen eines Graphen, dessen tats\"achliche
Community-Struktur nicht bekannt ist, ist der Modularity-Wert (siehe
Abschnitt \ref{sec:community-analyse}).  Um zu vermeiden, dass eine
vergleichsweise schlechte Einteilung durch einen Algorithmus die
Ergebnisse verf\"alscht, wurden mehrere Algorithmen zur
Community-Erkennung auf ungerichteten Graphen verwendet, um ihre
Ergebnisse vergleichen zu k\"onnen: Fast-Modularity (FM) von Newman et
al., sowie der Algorithmus von Blondel et al. (LP) (siehe Abschnitt
\ref{ch:Grundlagen:sec:Netzwerkanalyse:subsec:Communities}). Die
Auswahl wurde aus folgenden Gr\"unden getroffen:

\begin{itemize}
\item Fast-Modularity war einer der ersten praktikablen Algorithmen
  zur Community-Erkennung in grossen Netzwerken und wird weiterhin
  h\"aufig verwendet. Allerdings liefert er in der Literatur
  durchg\"angig schlechtere Ergebnisse als moderne Algorithmen
  \cite{Fortunato2010} und wird deshalb hier nur als Vergleichsbasis
  verwendet.
\item Der Algorithmus von Blondel et al. erzielte im Vergleich von
  Lancichinetti et al. \cite{Lancichinetti2009} mit die besten Ergebnisse.
\end{itemize}

Ein weiteres Auswahlkriterium war, dass f\"ur diese Algorithmen
Implementierungen \"offentlich mit Sourcecode verf\"ugbar
sind. Ausserdem existieren Algorithmen, die zwar f\"ur kleinere
Graphen bessere Ergebnisse liefern als die hier verwendeten, aufgrund
ihrer Berechnungskomplexit\"at f\"ur grosse Graphen wie den hier
vorliegenden aber nicht geeignet sind.

Eine Zuordnung von einer Person zu genau einer sozialen Gruppe scheint
unrealistisch. Statt dessen ist davon auszugehen, dass eine Person im
Allgemeinen zu mehreren Institutionen oder Gruppen zugeh\"orig
ist. Ebenso ist es m\"oglich, dass sie sich einerseits im Kontext
einer sozialen Gruppe vernetzt, andererseits aber auch an
Keysigning-Parties teilnimmt. Eine solche multiple Zugeh\"origkeit
sollte sich im Netzwerk durch eine Mitgliedschaft eines Knotens in
mehreren Communities ausdr\"ucken. Wie in
\ref{ch:Grundlagen:sec:Netzwerkanalyse:subsec:Communities} erw\"ahnt,
existiert eine Klasse von Algorithmen, die ein Netzwerk in
\emph{\"uberlappende} Communities zerlegt, so dass ein Knoten Teil von
mehreren Communities sein kann. Um zu untersuchen, inwiefern dieser
\"Uberlappungseffekt die Zuordnung von Knoten zu sozialen Gruppen und
KSPs beeinflusst, wurde zus\"atzlich noch eine \"uberlappende
Zerlegung mit dem Algorithmus COPRA \cite{Gregory2010}, einer
Verallgemeinerung von Label-Progagation auf berechnet. Dabei handelt
es sich um eine Verallgemeinerung des bereits erw\"ahnten Algorithmus
Label-Propagation. COPRA erwartet einen Parameter $v$, der angibt, zu
wie vielen Communities ein Knoten geh\"oren kann. Um hier das Optimum
bez\"uglich der Modularity zu finden, wurden Zerlegungen mit Werten
$v=2, \dots, 15$ berechnet. Zum Vergleich wurde eine Zerlegung mit
$v=1$ berechnet, die keine \"Uberlappung erlaubt. Da COPRA
nichtdeterministisch ist, wurden f\"ur jeden Wert 10 Berechnungen
durchgef\"uhrt, um die Abweichung zu ermitteln.

W\"ahrend Modularity die G\"ute einer Zerlegung nur bezogen auf die
Struktur des Graphen misst, dr\"ucken die Zuordnungszahlen die G\"ute
einer Partitionierung in Bezug auf die oben formulierte Hypothese
\"uber die Entstehung des Netzwerks aus. Wenn diese Hypothese
zutrifft, ist eine Zerlegung in Bezug darauf ``besser'', wenn eine
gr\"ossere Anzahl von Communities einer sozialen Gruppe oder
Keysigning-Party zugeordnet werden kann. Es kann nicht vorrausgesetzt
werden, dass eine Zerlegung mit hoher Modularity automatisch auch eine
Zerlegung mit hohen Zuordnungszahlen ist, dass sich also die real
existierenden Communities direkt in der Struktur des Graphen
wiederfinden.

\subsection{Communities in gerichteten Graphen}
\label{sec:comm-gericht-graph}
Der Graph des Web of Trust ist inh\"arent gerichtet, da eine Signatur
nur in eine Richtung gilt. Die meisten verf\"ugbaren Methoden zur
Community-Erkennung setzen im Gegensatz dazu einen ungerichteten
Graphen vorraus. Methoden, die auf gerichteten Graphen arbeiten, sind
erst in j\"ungster Zeit vorgestellt worden \cite{Leicht2008}
\cite{Rosvall2008}. Der bis jetzt am h\"aufigstpen verwendete Ansatz
f\"ur die Community-Erkennung in gerichteten Netzwerken ist es, die
Richtung der Kanten zu ignorieren und den Graphen als ungerichteten
Graphen zu behandeln. Auch die Arbeiten, die den Web of Trust-Graphen
unter Community-Gesichtspunkten analysieren oder ihn als Benchmark
f\"ur den Vergleich von Algorithmen verwenden, gehen so vor
\cite{Boguna2004}. Auch wenn dieses Vorgehen akzeptable Resultate
liefern kann, ist doch klar, dass durch das Ignorieren der Richtung
ein Informationsverlust stattfindet, der die Ergebnisse negativ
beeinflussen kann. In der vorliegenden Arbeit wurde dieser \"ubliche
Ansatz des Ignorierens der Kantenrichtung verwendet, da die Methoden
f\"ur ungerichtete Netzwerke besser etabliert und Implementationen
dieser Algorithmen einfacher zug\"anglich sind. Im Fall des Web of
Trust ist allerdings ein erheblicher Anteil der verbundenen
Knotenpaare nur in eine Richtung verbunden, d.h. es existiert keine
``Gegenkante'' (siehe Abschnitt
\ref{sec:result-allg-merkm-des}). Damit ist auch der
Informationsverlust durch die Reduzierung erheblich. Es ist daher
naheliegend, dass eine Community-Zerlegung des Graphen unter
Ber\"ucksichtigung der Richtungsinformationen eine Zerlegung berechnen
k\"onnte, die die Struktur des Graphen besser wiedergibt und auch
unter dem Gesichtspunkt der Zuteilung zu sozialen Gruppen und
Keysigning-Parties bessere Resultate liefert. Um den Einfluss der
Richtung zu \"uberpr\"ufen, wurde eine Zerlegung mit dem Algorithmus
``infomap'' von Rosvall et al. \cite{Rosvall2008} berechnet.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "diplarb"
%%% End: 
